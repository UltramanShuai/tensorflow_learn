{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu: tf.nn.relu()  \n",
    "sigmoid: tf.nn.sigmoid()  \n",
    "tanh: tf.nn.tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mse: tf.reduce_mean(tf.suqare(y-y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "SEED=23455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm=np.random.RandomState(SEED)\n",
    "X= rdm.rand(32,2)\n",
    "Y_=[[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Programmer\\Anaconda3\\envs\\python3_anaconda_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_=tf.placeholder(tf.float32,shape=(None,1))\n",
    "w1=tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y=tf.matmul(x,w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lose_mse=tf.reduce_mean(tf.square(y-y_))\n",
    "train_step=tf.train.GradientDescentOptimizer(0.001).minimize(lose_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), loss on all data is 0.655701\n",
      "w1 is:\n",
      " [[-0.80974597]\n",
      " [ 1.4852903 ]]\n",
      "After 500 training step(s), loss on all data is 0.35731\n",
      "w1 is:\n",
      " [[-0.46074435]\n",
      " [ 1.641878  ]]\n",
      "After 1000 training step(s), loss on all data is 0.232481\n",
      "w1 is:\n",
      " [[-0.21939856]\n",
      " [ 1.6984766 ]]\n",
      "After 1500 training step(s), loss on all data is 0.170404\n",
      "w1 is:\n",
      " [[-0.04415595]\n",
      " [ 1.7003176 ]]\n",
      "After 2000 training step(s), loss on all data is 0.133037\n",
      "w1 is:\n",
      " [[0.08942621]\n",
      " [1.673328  ]]\n",
      "After 2500 training step(s), loss on all data is 0.106939\n",
      "w1 is:\n",
      " [[0.19583555]\n",
      " [1.6322677 ]]\n",
      "After 3000 training step(s), loss on all data is 0.0870619\n",
      "w1 is:\n",
      " [[0.28375748]\n",
      " [1.5854434 ]]\n",
      "After 3500 training step(s), loss on all data is 0.0712709\n",
      "w1 is:\n",
      " [[0.35848638]\n",
      " [1.5374472 ]]\n",
      "After 4000 training step(s), loss on all data is 0.0584907\n",
      "w1 is:\n",
      " [[0.42332518]\n",
      " [1.4907393 ]]\n",
      "After 4500 training step(s), loss on all data is 0.0480653\n",
      "w1 is:\n",
      " [[0.48040026]\n",
      " [1.4465574 ]]\n",
      "After 5000 training step(s), loss on all data is 0.0395331\n",
      "w1 is:\n",
      " [[0.53113604]\n",
      " [1.4054536 ]]\n",
      "After 5500 training step(s), loss on all data is 0.0325409\n",
      "w1 is:\n",
      " [[0.5765325]\n",
      " [1.3675941]]\n",
      "After 6000 training step(s), loss on all data is 0.0268078\n",
      "w1 is:\n",
      " [[0.61732584]\n",
      " [1.3329403 ]]\n",
      "After 6500 training step(s), loss on all data is 0.0221059\n",
      "w1 is:\n",
      " [[0.6540846]\n",
      " [1.3013426]]\n",
      "After 7000 training step(s), loss on all data is 0.0182493\n",
      "w1 is:\n",
      " [[0.6872685]\n",
      " [1.272602 ]]\n",
      "After 7500 training step(s), loss on all data is 0.015086\n",
      "w1 is:\n",
      " [[0.71725976]\n",
      " [1.2465005 ]]\n",
      "After 8000 training step(s), loss on all data is 0.0124914\n",
      "w1 is:\n",
      " [[0.7443861]\n",
      " [1.2228197]]\n",
      "After 8500 training step(s), loss on all data is 0.0103631\n",
      "w1 is:\n",
      " [[0.7689324]\n",
      " [1.2013483]]\n",
      "After 9000 training step(s), loss on all data is 0.00861742\n",
      "w1 is:\n",
      " [[0.79115134]\n",
      " [1.1818889 ]]\n",
      "After 9500 training step(s), loss on all data is 0.00718553\n",
      "w1 is:\n",
      " [[0.811267 ]\n",
      " [1.1642567]]\n",
      "After 10000 training step(s), loss on all data is 0.006011\n",
      "w1 is:\n",
      " [[0.8294814]\n",
      " [1.1482829]]\n",
      "After 10500 training step(s), loss on all data is 0.00504758\n",
      "w1 is:\n",
      " [[0.84597576]\n",
      " [1.1338125 ]]\n",
      "After 11000 training step(s), loss on all data is 0.00425734\n",
      "w1 is:\n",
      " [[0.8609128]\n",
      " [1.1207061]]\n",
      "After 11500 training step(s), loss on all data is 0.00360914\n",
      "w1 is:\n",
      " [[0.87444043]\n",
      " [1.1088346 ]]\n",
      "After 12000 training step(s), loss on all data is 0.00307745\n",
      "w1 is:\n",
      " [[0.88669145]\n",
      " [1.0980824 ]]\n",
      "After 12500 training step(s), loss on all data is 0.00264134\n",
      "w1 is:\n",
      " [[0.8977863]\n",
      " [1.0883439]]\n",
      "After 13000 training step(s), loss on all data is 0.00228362\n",
      "w1 is:\n",
      " [[0.9078348]\n",
      " [1.0795243]]\n",
      "After 13500 training step(s), loss on all data is 0.00199021\n",
      "w1 is:\n",
      " [[0.91693527]\n",
      " [1.0715363 ]]\n",
      "After 14000 training step(s), loss on all data is 0.00174954\n",
      "w1 is:\n",
      " [[0.92517716]\n",
      " [1.0643018 ]]\n",
      "After 14500 training step(s), loss on all data is 0.00155213\n",
      "w1 is:\n",
      " [[0.93264157]\n",
      " [1.0577497 ]]\n",
      "After 15000 training step(s), loss on all data is 0.00139019\n",
      "w1 is:\n",
      " [[0.9394023]\n",
      " [1.0518153]]\n",
      "After 15500 training step(s), loss on all data is 0.00125737\n",
      "w1 is:\n",
      " [[0.9455251]\n",
      " [1.0464406]]\n",
      "After 16000 training step(s), loss on all data is 0.00114842\n",
      "w1 is:\n",
      " [[0.95107025]\n",
      " [1.0415728 ]]\n",
      "After 16500 training step(s), loss on all data is 0.00105905\n",
      "w1 is:\n",
      " [[0.9560928]\n",
      " [1.037164 ]]\n",
      "After 17000 training step(s), loss on all data is 0.000985753\n",
      "w1 is:\n",
      " [[0.96064115]\n",
      " [1.0331714 ]]\n",
      "After 17500 training step(s), loss on all data is 0.000925622\n",
      "w1 is:\n",
      " [[0.96476096]\n",
      " [1.0295546 ]]\n",
      "After 18000 training step(s), loss on all data is 0.00087631\n",
      "w1 is:\n",
      " [[0.9684917]\n",
      " [1.0262802]]\n",
      "After 18500 training step(s), loss on all data is 0.000835858\n",
      "w1 is:\n",
      " [[0.9718707]\n",
      " [1.0233142]]\n",
      "After 19000 training step(s), loss on all data is 0.000802676\n",
      "w1 is:\n",
      " [[0.974931 ]\n",
      " [1.0206276]]\n",
      "After 19500 training step(s), loss on all data is 0.000775461\n",
      "w1 is:\n",
      " [[0.9777026]\n",
      " [1.0181949]]\n",
      "Final w1 is :\n",
      " [[0.98019385]\n",
      " [1.0159807 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS=20000\n",
    "    for i in range(STEPS):\n",
    "        start=(i*BATCH_SIZE)%32\n",
    "        end=start+BATCH_SIZE\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})\n",
    "        if i %500==0:\n",
    "            total_loss= sess.run(lose_mse,feed_dict={x:X,y_:Y_})\n",
    "            print(\"After %d training step(s), loss on all data is %g\"%(i,total_loss))\n",
    "            print(\"w1 is:\\n\",sess.run(w1))\n",
    "    print(\"Final w1 is :\\n\",sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子： 预测销量，预测多了损失的是成本，少了，损失的是利润"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "SEED=23455\n",
    "COST= 9\n",
    "PROFIT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm=np.random.RandomState(SEED)\n",
    "X= rdm.rand(32,2)\n",
    "Y_=[[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_=tf.placeholder(tf.float32,shape=(None,1))\n",
    "w1=tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y=tf.matmul(x,w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*COST,(y_-y)*PROFIT))\n",
    "train_step=tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), loss on all data is 29.8745\n",
      "w1 is:\n",
      " [[-0.80594873]\n",
      " [ 1.4873729 ]]\n",
      "After 500 training step(s), loss on all data is 2.38814\n",
      "w1 is:\n",
      " [[0.8732146]\n",
      " [1.006204 ]]\n",
      "After 1000 training step(s), loss on all data is 1.46394\n",
      "w1 is:\n",
      " [[0.9658064]\n",
      " [0.9698208]]\n",
      "After 1500 training step(s), loss on all data is 1.46911\n",
      "w1 is:\n",
      " [[0.9645447]\n",
      " [0.9682946]]\n",
      "After 2000 training step(s), loss on all data is 1.43915\n",
      "w1 is:\n",
      " [[0.9602475]\n",
      " [0.9742084]]\n",
      "After 2500 training step(s), loss on all data is 1.45907\n",
      "w1 is:\n",
      " [[0.96100295]\n",
      " [0.96993417]]\n",
      "After 3000 training step(s), loss on all data is 1.43333\n",
      "w1 is:\n",
      " [[0.9654102]\n",
      " [0.9761159]]\n",
      "After 3500 training step(s), loss on all data is 1.43359\n",
      "w1 is:\n",
      " [[0.96414846]\n",
      " [0.9745897 ]]\n",
      "After 4000 training step(s), loss on all data is 1.44526\n",
      "w1 is:\n",
      " [[0.95985126]\n",
      " [0.9805035 ]]\n",
      "After 4500 training step(s), loss on all data is 1.46411\n",
      "w1 is:\n",
      " [[0.9636422]\n",
      " [0.9687893]]\n",
      "After 5000 training step(s), loss on all data is 1.44187\n",
      "w1 is:\n",
      " [[0.959345 ]\n",
      " [0.9747031]]\n",
      "After 5500 training step(s), loss on all data is 1.44656\n",
      "w1 is:\n",
      " [[0.9667877]\n",
      " [0.9734448]]\n",
      "After 6000 training step(s), loss on all data is 1.4717\n",
      "w1 is:\n",
      " [[0.9641995]\n",
      " [0.9676626]]\n",
      "After 6500 training step(s), loss on all data is 1.48756\n",
      "w1 is:\n",
      " [[0.9655712 ]\n",
      " [0.98128426]]\n",
      "After 7000 training step(s), loss on all data is 1.48998\n",
      "w1 is:\n",
      " [[0.9653083]\n",
      " [0.9817019]]\n",
      "After 7500 training step(s), loss on all data is 1.4512\n",
      "w1 is:\n",
      " [[0.9670821 ]\n",
      " [0.97273576]]\n",
      "After 8000 training step(s), loss on all data is 1.42806\n",
      "w1 is:\n",
      " [[0.964802 ]\n",
      " [0.9759015]]\n",
      "After 8500 training step(s), loss on all data is 1.48214\n",
      "w1 is:\n",
      " [[0.9658656]\n",
      " [0.9805752]]\n",
      "After 9000 training step(s), loss on all data is 1.45434\n",
      "w1 is:\n",
      " [[0.9646039]\n",
      " [0.979049 ]]\n",
      "After 9500 training step(s), loss on all data is 1.46374\n",
      "w1 is:\n",
      " [[0.96303403]\n",
      " [0.9685749 ]]\n",
      "After 10000 training step(s), loss on all data is 1.44686\n",
      "w1 is:\n",
      " [[0.95873684]\n",
      " [0.9744887 ]]\n",
      "After 10500 training step(s), loss on all data is 1.43406\n",
      "w1 is:\n",
      " [[0.9598004]\n",
      " [0.9791624]]\n",
      "After 11000 training step(s), loss on all data is 1.47155\n",
      "w1 is:\n",
      " [[0.966935 ]\n",
      " [0.9689562]]\n",
      "After 11500 training step(s), loss on all data is 1.4499\n",
      "w1 is:\n",
      " [[0.95929414]\n",
      " [0.97336197]]\n",
      "After 12000 training step(s), loss on all data is 1.46982\n",
      "w1 is:\n",
      " [[0.96004957]\n",
      " [0.9690877 ]]\n",
      "After 12500 training step(s), loss on all data is 1.43006\n",
      "w1 is:\n",
      " [[0.9600948 ]\n",
      " [0.97845334]]\n",
      "After 13000 training step(s), loss on all data is 1.4356\n",
      "w1 is:\n",
      " [[0.96085024]\n",
      " [0.9741791 ]]\n",
      "After 13500 training step(s), loss on all data is 1.45304\n",
      "w1 is:\n",
      " [[0.9649493 ]\n",
      " [0.97141284]]\n",
      "After 14000 training step(s), loss on all data is 1.47699\n",
      "w1 is:\n",
      " [[0.96499455]\n",
      " [0.98077846]]\n",
      "After 14500 training step(s), loss on all data is 1.4492\n",
      "w1 is:\n",
      " [[0.96373284]\n",
      " [0.9792523 ]]\n",
      "After 15000 training step(s), loss on all data is 1.46042\n",
      "w1 is:\n",
      " [[0.962163  ]\n",
      " [0.96877813]]\n",
      "After 15500 training step(s), loss on all data is 1.44943\n",
      "w1 is:\n",
      " [[0.9601911]\n",
      " [0.9808918]]\n",
      "After 16000 training step(s), loss on all data is 1.43981\n",
      "w1 is:\n",
      " [[0.95892936]\n",
      " [0.97936565]]\n",
      "After 16500 training step(s), loss on all data is 1.46822\n",
      "w1 is:\n",
      " [[0.9660639]\n",
      " [0.9691594]]\n",
      "After 17000 training step(s), loss on all data is 1.45746\n",
      "w1 is:\n",
      " [[0.9604402 ]\n",
      " [0.97081715]]\n",
      "After 17500 training step(s), loss on all data is 1.43351\n",
      "w1 is:\n",
      " [[0.96484745]\n",
      " [0.97699887]]\n",
      "After 18000 training step(s), loss on all data is 1.46748\n",
      "w1 is:\n",
      " [[0.9642764 ]\n",
      " [0.96846855]]\n",
      "After 18500 training step(s), loss on all data is 1.4398\n",
      "w1 is:\n",
      " [[0.9599792 ]\n",
      " [0.97438234]]\n",
      "After 19000 training step(s), loss on all data is 1.42141\n",
      "w1 is:\n",
      " [[0.9630599]\n",
      " [0.976308 ]]\n",
      "After 19500 training step(s), loss on all data is 1.44697\n",
      "w1 is:\n",
      " [[0.967159  ]\n",
      " [0.97354174]]\n",
      "Final w1 is :\n",
      " [[0.9661967 ]\n",
      " [0.97694933]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS=20000\n",
    "    for i in range(STEPS):\n",
    "        start=(i*BATCH_SIZE)%32\n",
    "        end=start+BATCH_SIZE\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})\n",
    "        if i %500==0:\n",
    "            total_loss= sess.run(loss,feed_dict={x:X,y_:Y_})\n",
    "            print(\"After %d training step(s), loss on all data is %g\"%(i,total_loss))\n",
    "            print(\"w1 is:\\n\",sess.run(w1))\n",
    "    print(\"Final w1 is :\\n\",sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_entropy: -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-12,1.0)))  \n",
    "tf.clip_by_value() y小于1e-12为1e-12 ，大于1 为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-12 防止出现log0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实际用以下函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入 softmax() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,lables=tf.argmax(y_,1))  \n",
    "cem= tf.reduce_mean(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
